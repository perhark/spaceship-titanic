{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:01:34] WARNING: ../src/learner.cc:627: \n",
      "Parameters: { \"verbose\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "{'n_estimators': 453, 'min_child_weight': 1, 'max_depth': 15, 'learning_rate': 0.05, 'gamma': 0.2, 'colsample_bytree': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/lib/python3.7/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.70018 | train_auc: 0.70585 | valid_auc: 0.66829 |  0:00:05s\n",
      "epoch 1  | loss: 0.61262 | train_auc: 0.75813 | valid_auc: 0.73304 |  0:00:11s\n",
      "epoch 2  | loss: 0.57045 | train_auc: 0.8055  | valid_auc: 0.78089 |  0:00:16s\n",
      "epoch 3  | loss: 0.54513 | train_auc: 0.82697 | valid_auc: 0.80113 |  0:00:21s\n",
      "epoch 4  | loss: 0.53711 | train_auc: 0.84074 | valid_auc: 0.81472 |  0:00:27s\n",
      "epoch 5  | loss: 0.51905 | train_auc: 0.8497  | valid_auc: 0.82351 |  0:00:32s\n",
      "epoch 6  | loss: 0.51527 | train_auc: 0.85631 | valid_auc: 0.8305  |  0:00:38s\n",
      "epoch 7  | loss: 0.4935  | train_auc: 0.86232 | valid_auc: 0.83723 |  0:00:43s\n",
      "epoch 8  | loss: 0.49431 | train_auc: 0.86614 | valid_auc: 0.83787 |  0:00:48s\n",
      "epoch 9  | loss: 0.48221 | train_auc: 0.8676  | valid_auc: 0.84162 |  0:00:54s\n",
      "epoch 10 | loss: 0.4863  | train_auc: 0.86925 | valid_auc: 0.84364 |  0:01:00s\n",
      "epoch 11 | loss: 0.4747  | train_auc: 0.87092 | valid_auc: 0.84584 |  0:01:05s\n",
      "epoch 12 | loss: 0.47561 | train_auc: 0.87397 | valid_auc: 0.85118 |  0:01:11s\n",
      "epoch 13 | loss: 0.46854 | train_auc: 0.87452 | valid_auc: 0.84862 |  0:01:16s\n",
      "epoch 14 | loss: 0.47184 | train_auc: 0.87651 | valid_auc: 0.85045 |  0:01:21s\n",
      "epoch 15 | loss: 0.47557 | train_auc: 0.87728 | valid_auc: 0.85071 |  0:01:27s\n",
      "epoch 16 | loss: 0.4656  | train_auc: 0.87735 | valid_auc: 0.85032 |  0:01:32s\n",
      "epoch 17 | loss: 0.46597 | train_auc: 0.87937 | valid_auc: 0.85409 |  0:01:38s\n",
      "epoch 18 | loss: 0.46377 | train_auc: 0.87827 | valid_auc: 0.85565 |  0:01:43s\n",
      "epoch 19 | loss: 0.45639 | train_auc: 0.87499 | valid_auc: 0.85264 |  0:01:49s\n",
      "epoch 20 | loss: 0.45833 | train_auc: 0.87647 | valid_auc: 0.85276 |  0:01:55s\n",
      "epoch 21 | loss: 0.45738 | train_auc: 0.88106 | valid_auc: 0.8563  |  0:02:00s\n",
      "epoch 22 | loss: 0.45735 | train_auc: 0.88017 | valid_auc: 0.85661 |  0:02:06s\n",
      "epoch 23 | loss: 0.45082 | train_auc: 0.88369 | valid_auc: 0.85813 |  0:02:11s\n",
      "epoch 24 | loss: 0.45993 | train_auc: 0.88297 | valid_auc: 0.85763 |  0:02:17s\n",
      "epoch 25 | loss: 0.45329 | train_auc: 0.88176 | valid_auc: 0.85858 |  0:02:22s\n",
      "epoch 26 | loss: 0.45321 | train_auc: 0.88448 | valid_auc: 0.86043 |  0:02:28s\n",
      "epoch 27 | loss: 0.44941 | train_auc: 0.88467 | valid_auc: 0.86066 |  0:02:33s\n",
      "epoch 28 | loss: 0.44638 | train_auc: 0.88788 | valid_auc: 0.86382 |  0:02:39s\n",
      "epoch 29 | loss: 0.45024 | train_auc: 0.8845  | valid_auc: 0.85916 |  0:02:44s\n",
      "epoch 30 | loss: 0.45205 | train_auc: 0.88459 | valid_auc: 0.85995 |  0:02:50s\n",
      "epoch 31 | loss: 0.44722 | train_auc: 0.88589 | valid_auc: 0.86011 |  0:02:56s\n",
      "epoch 32 | loss: 0.44615 | train_auc: 0.88616 | valid_auc: 0.86218 |  0:03:01s\n",
      "epoch 33 | loss: 0.45358 | train_auc: 0.88766 | valid_auc: 0.86289 |  0:03:07s\n",
      "epoch 34 | loss: 0.44398 | train_auc: 0.88738 | valid_auc: 0.86142 |  0:03:12s\n",
      "epoch 35 | loss: 0.44211 | train_auc: 0.88784 | valid_auc: 0.86017 |  0:03:18s\n",
      "epoch 36 | loss: 0.44531 | train_auc: 0.88877 | valid_auc: 0.8646  |  0:03:24s\n",
      "epoch 37 | loss: 0.44142 | train_auc: 0.88663 | valid_auc: 0.86065 |  0:03:29s\n",
      "epoch 38 | loss: 0.44489 | train_auc: 0.88777 | valid_auc: 0.86441 |  0:03:35s\n",
      "epoch 39 | loss: 0.44282 | train_auc: 0.88912 | valid_auc: 0.8652  |  0:03:40s\n",
      "epoch 40 | loss: 0.44216 | train_auc: 0.88955 | valid_auc: 0.86627 |  0:03:46s\n",
      "epoch 41 | loss: 0.43309 | train_auc: 0.89134 | valid_auc: 0.86662 |  0:03:51s\n",
      "epoch 42 | loss: 0.44196 | train_auc: 0.88793 | valid_auc: 0.86208 |  0:03:57s\n",
      "epoch 43 | loss: 0.44179 | train_auc: 0.89045 | valid_auc: 0.86402 |  0:04:03s\n",
      "epoch 44 | loss: 0.44256 | train_auc: 0.89021 | valid_auc: 0.86628 |  0:04:08s\n",
      "epoch 45 | loss: 0.43993 | train_auc: 0.89036 | valid_auc: 0.86455 |  0:04:14s\n",
      "epoch 46 | loss: 0.43774 | train_auc: 0.88933 | valid_auc: 0.86334 |  0:04:20s\n",
      "epoch 47 | loss: 0.43578 | train_auc: 0.89168 | valid_auc: 0.86694 |  0:04:25s\n",
      "epoch 48 | loss: 0.43802 | train_auc: 0.89174 | valid_auc: 0.86569 |  0:04:31s\n",
      "epoch 49 | loss: 0.44362 | train_auc: 0.89175 | valid_auc: 0.86681 |  0:04:37s\n",
      "epoch 50 | loss: 0.44058 | train_auc: 0.89276 | valid_auc: 0.8669  |  0:04:43s\n",
      "epoch 51 | loss: 0.43465 | train_auc: 0.89276 | valid_auc: 0.86638 |  0:04:49s\n",
      "epoch 52 | loss: 0.43852 | train_auc: 0.89196 | valid_auc: 0.86654 |  0:04:55s\n",
      "epoch 53 | loss: 0.43212 | train_auc: 0.89357 | valid_auc: 0.86761 |  0:05:01s\n",
      "epoch 54 | loss: 0.4375  | train_auc: 0.894   | valid_auc: 0.86881 |  0:05:07s\n",
      "epoch 55 | loss: 0.42616 | train_auc: 0.89526 | valid_auc: 0.86905 |  0:05:13s\n",
      "epoch 56 | loss: 0.43565 | train_auc: 0.89466 | valid_auc: 0.86687 |  0:05:18s\n",
      "epoch 57 | loss: 0.43234 | train_auc: 0.89011 | valid_auc: 0.86453 |  0:05:24s\n",
      "epoch 58 | loss: 0.44058 | train_auc: 0.88968 | valid_auc: 0.86421 |  0:05:30s\n",
      "epoch 59 | loss: 0.43752 | train_auc: 0.89103 | valid_auc: 0.86518 |  0:05:35s\n",
      "epoch 60 | loss: 0.43781 | train_auc: 0.89286 | valid_auc: 0.86589 |  0:05:41s\n",
      "epoch 61 | loss: 0.4356  | train_auc: 0.89315 | valid_auc: 0.86651 |  0:05:46s\n",
      "epoch 62 | loss: 0.44212 | train_auc: 0.8953  | valid_auc: 0.87042 |  0:05:52s\n",
      "epoch 63 | loss: 0.4318  | train_auc: 0.89359 | valid_auc: 0.86908 |  0:05:57s\n",
      "epoch 64 | loss: 0.43172 | train_auc: 0.89419 | valid_auc: 0.86872 |  0:06:03s\n",
      "epoch 65 | loss: 0.42687 | train_auc: 0.89524 | valid_auc: 0.86964 |  0:06:08s\n",
      "epoch 66 | loss: 0.43007 | train_auc: 0.89548 | valid_auc: 0.87051 |  0:06:14s\n",
      "epoch 67 | loss: 0.43144 | train_auc: 0.89708 | valid_auc: 0.8708  |  0:06:20s\n",
      "epoch 68 | loss: 0.43345 | train_auc: 0.89539 | valid_auc: 0.86867 |  0:06:25s\n",
      "epoch 69 | loss: 0.43082 | train_auc: 0.89664 | valid_auc: 0.86952 |  0:06:31s\n",
      "epoch 70 | loss: 0.43513 | train_auc: 0.89458 | valid_auc: 0.86728 |  0:06:36s\n",
      "epoch 71 | loss: 0.4284  | train_auc: 0.89496 | valid_auc: 0.86753 |  0:06:42s\n",
      "epoch 72 | loss: 0.429   | train_auc: 0.89715 | valid_auc: 0.87103 |  0:06:47s\n",
      "epoch 73 | loss: 0.424   | train_auc: 0.89911 | valid_auc: 0.872   |  0:06:53s\n",
      "epoch 74 | loss: 0.42961 | train_auc: 0.89897 | valid_auc: 0.87171 |  0:06:58s\n",
      "epoch 75 | loss: 0.42982 | train_auc: 0.89973 | valid_auc: 0.87247 |  0:07:04s\n",
      "epoch 76 | loss: 0.42668 | train_auc: 0.90028 | valid_auc: 0.87286 |  0:07:09s\n",
      "epoch 77 | loss: 0.42528 | train_auc: 0.89808 | valid_auc: 0.87    |  0:07:15s\n",
      "epoch 78 | loss: 0.42592 | train_auc: 0.89823 | valid_auc: 0.87005 |  0:07:20s\n",
      "epoch 79 | loss: 0.42519 | train_auc: 0.89938 | valid_auc: 0.87043 |  0:07:26s\n",
      "epoch 80 | loss: 0.42693 | train_auc: 0.90043 | valid_auc: 0.87009 |  0:07:32s\n",
      "epoch 81 | loss: 0.42383 | train_auc: 0.90046 | valid_auc: 0.87096 |  0:07:38s\n",
      "epoch 82 | loss: 0.42417 | train_auc: 0.90013 | valid_auc: 0.87131 |  0:07:44s\n",
      "epoch 83 | loss: 0.42574 | train_auc: 0.89841 | valid_auc: 0.87008 |  0:07:49s\n",
      "epoch 84 | loss: 0.42279 | train_auc: 0.89821 | valid_auc: 0.87016 |  0:07:55s\n",
      "epoch 85 | loss: 0.43229 | train_auc: 0.89855 | valid_auc: 0.87032 |  0:08:01s\n",
      "epoch 86 | loss: 0.42621 | train_auc: 0.8992  | valid_auc: 0.87052 |  0:08:07s\n",
      "\n",
      "Early stopping occurred at epoch 86 with best_epoch = 76 and best_valid_auc = 0.87286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/lib/python3.7/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    8.0s remaining:    3.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   11.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 446, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 15, 'bootstrap': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joseph/anaconda3/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'iterations': 500, 'depth': 10}\n",
      "XGBOOST\n",
      "0.999616515403298\n",
      "0.7781609195402299\n",
      "Decision tree\n",
      "0.9126933401508373\n",
      "0.7942528735632184\n",
      "catBOOST\n",
      "0.8646299373641826\n",
      "0.7793103448275862\n"
     ]
    }
   ],
   "source": [
    "#Link:https://www.kaggle.com/competitions/spaceship-titanic/leaderboard\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def convert_data(ofus_matrix):\n",
    "\n",
    "    matrix = np.zeros(ofus_matrix.shape) -1\n",
    "\n",
    "    for i in range(ofus_matrix.shape[1]):\n",
    "        attributes = ofus_matrix[:,i]\n",
    "        if ( isinstance(attributes[0], (int, float)) == False):\n",
    "            le = LabelEncoder()\n",
    "            le.fit(attributes)\n",
    "            matrix[:,i] = le.transform(attributes)\n",
    "        else:\n",
    "            matrix[:,i] = np.nan_to_num(attributes)\n",
    "    return matrix\n",
    "\n",
    "def normalize(ofus_matrix):\n",
    "    \n",
    "    matrix = np.zeros(ofus_matrix.shape) -1\n",
    "    for i in range(ofus_matrix.shape[1]):\n",
    "        attributes = ofus_matrix[:,i]\n",
    "        scaler = preprocessing.MinMaxScaler()\n",
    "        matrix[:,i]=scaler.fit_transform(attributes.reshape([-1,1])).reshape(attributes.shape)\n",
    "    return matrix\n",
    "\n",
    "def convert(train,test):\n",
    "    r, c = train.shape\n",
    "    rt, ct = test.shape\n",
    "    result = pd.concat([train,test])\n",
    "    new_result= convert_data(result.to_numpy())\n",
    "    #new_result = normalize(new_result)\n",
    "    new_train =new_result[0:r,]\n",
    "    new_test = new_result[r:r+rt,]\n",
    "    \n",
    "    return (new_train,new_test)\n",
    "    \n",
    "\n",
    "def pre (train,test) :\n",
    "    numbers_train = train.select_dtypes(np.number)\n",
    "    object_train = train.select_dtypes(exclude=[np.number]) #bool and strings\n",
    "    \n",
    "    #fill nans with moda and median TRAIN\n",
    "    for numbers in numbers_train:\n",
    "         train.loc[:,numbers] = train.loc[:,numbers].fillna(train.loc[:,numbers].median())\n",
    "            \n",
    "    for obj in object_train:\n",
    "         train.loc[:,obj] = train.loc[:,obj].fillna(train.loc[:,obj].mode()[0])\n",
    "            \n",
    "            \n",
    "    numbers_test = test.select_dtypes(np.number)\n",
    "    object_test = test.select_dtypes(exclude=[np.number]) #bool and strings\n",
    "    \n",
    "    #fill nans with moda and median TRAIN\n",
    "    for numbers in numbers_test:\n",
    "         test.loc[:,numbers] = test.loc[:,numbers].fillna(test.loc[:,numbers].median())\n",
    "\n",
    "    for obj in object_test:\n",
    "         test.loc[:,obj] = test.loc[:,obj].fillna(test.loc[:,obj].mode()[0])\n",
    "    \n",
    "    # Adding group and number columns to test and train\n",
    "    \n",
    "    train['group'] = train.PassengerId.str.extract( r'(\\d+)\\_', expand=False)\n",
    "    train['number'] = train.PassengerId.str.extract( r'\\_(\\d+)', expand=False)\n",
    "\n",
    "    test['group'] = test.PassengerId.str.extract( r'(\\d+)\\_', expand=False)\n",
    "    test['number'] = test.PassengerId.str.extract( r'\\_(\\d+)', expand=False)\n",
    "    \n",
    "    train = train.drop(['PassengerId'], axis=1)\n",
    "    test = test.drop(['PassengerId'], axis=1)\n",
    "    \n",
    "    #adding deck,num and side columns to test and train\n",
    "    \n",
    "    train['deck'] = train.Cabin.str.extract( '([A-Za-z]+)\\/', expand=False)\n",
    "    train['num'] = train.Cabin.str.extract( r'(\\d+)', expand=False)\n",
    "    train['side'] = train.Cabin.str.extract( '\\/([A-Za-z]+)', expand=False)\n",
    "    \n",
    "    test['deck'] = test.Cabin.str.extract( '([A-Za-z]+)\\/', expand=False)\n",
    "    test['num'] = test.Cabin.str.extract( r'(\\d+)', expand=False)\n",
    "    test['side'] = test.Cabin.str.extract( '\\/([A-Za-z]+)', expand=False)\n",
    "\n",
    "    train = train.drop(['Cabin'], axis=1)\n",
    "    test = test.drop(['Cabin'], axis=1)\n",
    "    \n",
    "    #NAME\n",
    "    train['Name'] = train.Name.str.extract( '(\\ [A-Za-z]+)', expand=False)\n",
    "    test['Name'] = test.Name.str.extract( '(\\ [A-Za-z]+)', expand=False)\n",
    "    \n",
    "    #AGE \n",
    "    \n",
    "    train.loc[train['Age']<22,'Age'] = 1\n",
    "    train.loc[ (train['Age']>=22) & (train['Age']<=33) ,'Age'] = 2\n",
    "    train.loc[ (train['Age']>33)  ,'Age'] = 3\n",
    "    train['Age'] = train['Age'].astype(int)\n",
    "\n",
    "    test.loc[test['Age']<22,'Age'] = 1\n",
    "    test.loc[ (test['Age']>=22) & (test['Age']<=33) ,'Age'] = 2\n",
    "    test.loc[ (test['Age']>33) ,'Age'] = 3\n",
    "    test['Age'] = test['Age'].astype(int)\n",
    "\n",
    "    \"\"\"\n",
    "    #Roomservice\n",
    "    \n",
    "    #erase transported column\n",
    "    train_aux = train.copy()\n",
    "    train_aux  = train_aux.drop(['Transported'], axis=1)\n",
    "\n",
    "    #join all data\n",
    "    #result = pd.concat([train_aux,test])\n",
    "\n",
    "    train.loc[train['RoomService']<1.0,'RoomService'] = 0\n",
    "    train.loc[train['RoomService']>=1.0 ,'RoomService'] = 1\n",
    "    \n",
    "    test.loc[test['RoomService']<1.0,'RoomService'] = 0\n",
    "    test.loc[test['RoomService']>=1.0 ,'RoomService'] = 1\n",
    "    \n",
    "    train['RoomService'] = train['RoomService'].astype(int)\n",
    "    test['RoomService'] = test['RoomService'].astype(int)\n",
    "    \n",
    "    #FoodCourt\n",
    "    \n",
    "    train.loc[train['FoodCourt']<2.0,'FoodCourt'] = 0\n",
    "    train.loc[train['FoodCourt']>=2.0 ,'FoodCourt'] = 1\n",
    "    \n",
    "    test.loc[test['FoodCourt']<2.0,'FoodCourt'] = 0\n",
    "    test.loc[test['FoodCourt']>=2.0 ,'FoodCourt'] = 1\n",
    "    \n",
    "    train['FoodCourt'] = train['FoodCourt'].astype(int)\n",
    "    test['FoodCourt'] = test['FoodCourt'].astype(int)\n",
    "    \n",
    "    #ShoppingMall\n",
    "    train.loc[train['ShoppingMall']<0.667,'ShoppingMall'] = 0\n",
    "    train.loc[train['ShoppingMall']>=0.667 ,'ShoppingMall'] = 1\n",
    "    \n",
    "    test.loc[test['ShoppingMall']<0.667,'ShoppingMall'] = 0\n",
    "    test.loc[test['ShoppingMall']>=0.667 ,'ShoppingMall'] = 1\n",
    "    \n",
    "    train['ShoppingMall'] = train['ShoppingMall'].astype(int)\n",
    "    test['ShoppingMall'] = test['ShoppingMall'].astype(int)\n",
    "    \n",
    "    #Spa\n",
    "    \n",
    "    train.loc[train['Spa']<3,'Spa'] = 0\n",
    "    train.loc[train['Spa']>=3 ,'Spa'] = 1\n",
    "    \n",
    "    test.loc[test['Spa']<3,'Spa'] = 0\n",
    "    test.loc[test['Spa']>=3,'Spa'] = 1\n",
    "    \n",
    "    train['Spa'] = train['Spa'].astype(int)\n",
    "    test['Spa'] = test['Spa'].astype(int)\n",
    "    \n",
    "    #VRDeck\n",
    "    train.loc[train['VRDeck']<1,'VRDeck'] = 0\n",
    "    train.loc[train['VRDeck']>=1 ,'VRDeck'] = 1\n",
    "    \n",
    "    test.loc[test['VRDeck']<1,'VRDeck'] = 0\n",
    "    test.loc[test['VRDeck']>=1 ,'VRDeck'] = 1\n",
    "    \n",
    "    train['VRDeck'] = train['VRDeck'].astype(int)\n",
    "    test['VRDeck'] = test['VRDeck'].astype(int)\n",
    "    \n",
    "    \"\"\"\n",
    "    return (train,test)\n",
    "\n",
    "\n",
    "def fine_tune_xgb_sklearn(X_train, Y_train, seed):\n",
    "    RS_CV = 5\n",
    "    RS_N_ITER = 2\n",
    "    RS_N_JOBS = -1\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 50, stop = 2000, num = 30)]\n",
    "                    \n",
    "    \n",
    "    random_grid = {\n",
    "                 'learning_rate' : [0.05,0.10,0.15,0.20,0.25,0.30],\n",
    "                 'max_depth' : [ 3, 4, 5, 6, 8, 10,15],\n",
    "                 'min_child_weight' : [ 1, 3, 5, 7,9],\n",
    "                 'gamma': [ 0.0, 0.1, 0.2 , 0.3, 0.4 ,1],\n",
    "                 'colsample_bytree': [ 0.3, 0.4, 0.5 ,0.6,0.7,1],\n",
    "                 'n_estimators': n_estimators}\n",
    "    \n",
    "     \n",
    "    clf_xgb = XGBClassifier(#n_estimators =3000,\n",
    "                            verbosity=1,\n",
    "                            objective='binary:logistic',\n",
    "                            booster='gbtree',\n",
    "                            #n_jobs=-1,\n",
    "                            #nthread=None,\n",
    "                            #max_delta_step=0,\n",
    "                            subsample=0.7,\n",
    "                            #colsample_bylevel=1,\n",
    "                            #colsample_bynode=1,\n",
    "                            #reg_alpha=0,\n",
    "                            #reg_lambda=1,\n",
    "                            #scale_pos_weight=1,\n",
    "                            #base_score=0.5,\n",
    "                            #random_state=0,\n",
    "                            verbose=0,\n",
    "                            #seed=None\n",
    "                           )\n",
    "\n",
    "    \n",
    "    clf_random = RandomizedSearchCV(estimator = clf_xgb,\n",
    "                                    param_distributions = random_grid,\n",
    "                                    n_iter = RS_N_ITER, \n",
    "                                    cv = RS_CV,\n",
    "                                    verbose=0,\n",
    "                                    random_state=seed, n_jobs = RS_N_JOBS)\n",
    "    \n",
    "    clf_random.fit(X_train, Y_train.astype(int))\n",
    "    print(clf_random.best_params_)\n",
    "    return clf_random\n",
    "\n",
    "def tab_net(trainx,trainy,seed):\n",
    "    X_train, X_valid, Y_train,Y_valid = train_test_split(trainx, trainy, test_size = 0.25, random_state =seed,stratify=trainy)\n",
    "    \n",
    "    tb_cls = TabNetClassifier(optimizer_fn=torch.optim.Adam,\n",
    "                               optimizer_params=dict(lr=1e-3),\n",
    "                               scheduler_params={\"step_size\":10, \"gamma\":0.9},\n",
    "                               scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "                               verbose=1,\n",
    "                               seed=seed,\n",
    "                               mask_type='entmax' # \"sparsemax\" entmax\n",
    "                               )\n",
    "\n",
    "    tb_cls.fit(X_train,Y_train,\n",
    "                               eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
    "                               eval_name=['train', 'valid'],\n",
    "                               eval_metric=['auc'],\n",
    "                               max_epochs=100 , patience=10,\n",
    "                               batch_size=32, drop_last=False)    \n",
    "    return tb_cls\n",
    "\n",
    "def fine_tune_RCF_sklearn(X_train, Y_train, seed):\n",
    "    n_estimators = [int(x) for x in np.linspace(start = 400, stop = 700, num = 14)]\n",
    "    max_features = ['sqrt','log2']\n",
    "    max_depth = [int(x) for x in np.linspace(10, 55, num = 10)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2, 5, 10]\n",
    "    min_samples_leaf = [1, 2, 4]\n",
    "    bootstrap = [True, False]\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "    \n",
    "    clf = RandomForestClassifier(random_state = seed)\n",
    "    clf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 2, cv = 5, verbose=2, random_state=seed, n_jobs = -1)\n",
    "    clf_random.fit(X_train, Y_train.astype(int))\n",
    "    print(clf_random.best_params_)\n",
    "    return clf_random\n",
    "    \n",
    "def fine_tune_cat_sklearn(X_train, Y_train, seed):\n",
    "    \n",
    "    RS_CV = 5 \n",
    "    RS_N_ITER = 2\n",
    "    RS_N_JOBS = -1\n",
    "\n",
    "    iterations = [int(x) for x in np.linspace(start = 500, stop = 2000, num = 5)]\n",
    "    \n",
    "    random_grid = {\n",
    "                   'depth':[8,10],\n",
    "                   'learning_rate': [0.1,0.01],\n",
    "                   'iterations': iterations\n",
    "                   }\n",
    "    \n",
    "    clf = CatBoostClassifier(\n",
    "                            #iterations=1000,         # Reduced iterations\n",
    "                            l2_leaf_reg=3.0,         # Increased L2 regularization term\n",
    "                            #eval_metric='Accuracy',\n",
    "                            random_seed=seed,\n",
    "                            verbose=0,\n",
    "                            #loss_function ='accuracy'\n",
    "                           )\n",
    "\n",
    "    clf_random = RandomizedSearchCV(estimator = clf,\n",
    "                                    param_distributions = random_grid,\n",
    "                                    n_iter = RS_N_ITER, \n",
    "                                    cv = RS_CV,\n",
    "                                    verbose=0, random_state=seed, n_jobs = RS_N_JOBS)\n",
    "    \n",
    "    clf_random.fit(X_train, Y_train.astype(int))\n",
    "    print(clf_random.best_params_)\n",
    "    return clf_random\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "submission = pd.read_csv('data/sub.csv')\n",
    "\n",
    "\n",
    "(train,test) = pre(train,test)\n",
    "\n",
    "\n",
    "Y_train = train['Transported'].to_numpy()\n",
    "train  = train.drop(['Transported'], axis=1)\n",
    "\n",
    "(X_train,X_test)=convert(train,test)\n",
    "\n",
    "x=X_train.copy()\n",
    "y=Y_train.copy()\n",
    "seed = 13\n",
    "X_train, X_valid, Y_train,Y_valid = train_test_split(X_train, Y_train, test_size = 0.1, random_state =seed,stratify=Y_train)\n",
    "\n",
    "model1 = fine_tune_xgb_sklearn ( X_train,Y_train, seed)\n",
    "model2 = tab_net(x,y,seed)\n",
    "model3 = fine_tune_RCF_sklearn ( X_train,Y_train, seed)\n",
    "model4 = fine_tune_cat_sklearn ( X_train,Y_train, seed)\n",
    "\n",
    "print(\"XGBOOST\")\n",
    "print(model1.score(X_train,Y_train))\n",
    "print(model1.score(X_valid,Y_valid))\n",
    "\n",
    "print(\"Decision tree\")\n",
    "print(model3.score(X_train,Y_train))\n",
    "print(model3.score(X_valid,Y_valid))\n",
    "\n",
    "print(\"catBOOST\")\n",
    "print(model4.score(X_train,Y_train))\n",
    "print(model4.score(X_valid,Y_valid))\n",
    "\n",
    "result1= model1.predict(X_test)\n",
    "result2= model2.predict(X_test)\n",
    "result3= model3.predict(X_test)\n",
    "result4= model4.predict(X_test)\n",
    "\n",
    "submission['Transported'] =result1\n",
    "submission['Transported'] =submission['Transported'].replace(1,True)\n",
    "submission['Transported'] =submission['Transported'].replace(0,False)\n",
    "name = 'submission01.csv'\n",
    "submission.to_csv(name,index=False)\n",
    "\n",
    "submission['Transported'] =result2\n",
    "submission['Transported'] =submission['Transported'].replace(1,True)\n",
    "submission['Transported'] =submission['Transported'].replace(0,False)\n",
    "name = 'submission02.csv'\n",
    "submission.to_csv(name,index=False)\n",
    "\n",
    "submission['Transported'] =result3\n",
    "submission['Transported'] =submission['Transported'].replace(1,True)\n",
    "submission['Transported'] =submission['Transported'].replace(0,False)\n",
    "name = 'submission03.csv'\n",
    "submission.to_csv(name,index=False)\n",
    "\n",
    "submission['Transported'] =result4\n",
    "submission['Transported'] =submission['Transported'].replace(1,True)\n",
    "submission['Transported'] =submission['Transported'].replace(0,False)\n",
    "name = 'submission04.csv'\n",
    "submission.to_csv(name,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
